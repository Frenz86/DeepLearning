{"cells":[{"cell_type":"markdown","source":["<img src= \"https://frenzy86.s3.eu-west-2.amazonaws.com/python/safetensor.png\" width=1000>"],"metadata":{"id":"8yhTl3RQkoXw"}},{"cell_type":"markdown","source":["# Safetensors: a simple, safe and faster way to store and distribute tensors.\n","\n","Safetensors is a new simple format for storing tensors safely (as opposed to pickle) and that is still fast (zero-copy). Safetensors is really fast\n","\n","safetensors and ONNX serve different purposessafetensors is a simple, safe, and fast file format for storing and loading tensors. It is a secure alternative to Python’s pickle utility, which is not secure and may contain malicious code that can be executed."],"metadata":{"id":"bpmVB6Dpil0y"}},{"cell_type":"markdown","source":["In summary, safetensors is used for storing and loading tensors in a safe and fast way, while ONNX is used for sharing models between different deep learning frameworks. Same applies for other model sharing frameworks.\n","\n","https://medium.com/@mandalsouvik/safetensors-a-simple-and-safe-way-to-store-and-distribute-tensors-d9ba1931ba04"],"metadata":{"id":"TzUKT_k-iy7u"}},{"cell_type":"code","source":["import torch\n","from safetensors.torch import save_file\n","\n","tensors = {\n","            \"embedding\": torch.zeros((2, 2)),\n","            \"attention\": torch.zeros((2, 3))\n","            }\n","save_file(tensors, \"model.safetensors\")"],"metadata":{"id":"MeeoaoRximhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from safetensors import safe_open\n","\n","tensors = {}\n","with safe_open(\"model.safetensors\", framework=\"pt\", device=0) as f:\n","    for k in f.keys():\n","        tensors[k] = f.get_tensor(k) # loads the full tensor given a key\n","print(tensors)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzAnfYv8i8x6","executionInfo":{"status":"ok","timestamp":1734631039116,"user_tz":-60,"elapsed":289,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"a4596c30-ef24-4daa-e7bd-6e809a691196"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'attention': tensor([[0., 0., 0.],\n","        [0., 0., 0.]], device='cuda:0'), 'embedding': tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')}\n"]}]},{"cell_type":"markdown","source":["## Lazy loading\n","\n","Lazy loading is the ability to load only some tensors, or part of tensors for a given file. This is possible with safetensors.\n","\n","Lazy loading is really important in cases when we have a large file containing many key and value pairs. This can be a metadata cache for large dataset. If we can load the value for single keys indivisually it will be memory efficient and faster else we will have to load the full file into memory to inspect any of the key."],"metadata":{"id":"4xyPC_nUjZ6t"}},{"cell_type":"code","source":["#do not run\n","# from safetensors.torch import load_model, save_model\n","\n","# save_model(model, \"model1.safetensors\")\n","# # Instead of save_file(model.state_dict(), \"model.safetensors\")\n","\n","# load_model(model, \"model1.safetensors\")\n","# # Instead of model.load_state_dict(load_file(\"model.safetensors\"))"],"metadata":{"id":"11Y0YsfxjfCD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##PyTorch Operations\n","Load a model in PyTorch.\n","\n"],"metadata":{"id":"swuVAhTIj5_L"}},{"cell_type":"code","source":["from torchvision.models import resnet18\n","\n","model_pt = resnet18(pretrained=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRj1ILT0j6ny","executionInfo":{"status":"ok","timestamp":1734631057498,"user_tz":-60,"elapsed":6962,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"661ca905-4663-46e0-ee7e-6bd2cc7e1ae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 178MB/s]\n"]}]},{"cell_type":"markdown","source":["Save the state_dict to safetensor, and load them back to a new model.\n","\n"],"metadata":{"id":"Ii0VAgvTkJyz"}},{"cell_type":"code","source":["from safetensors.torch import load_model, save_model\n","\n","# save the state dict\n","save_model(model_pt, \"resnet18.safetensors\")\n","\n","# load the model without weights\n","model_st = resnet18(pretrained=False)\n","load_model(model_st, \"resnet18.safetensors\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLqnYJARkH5U","executionInfo":{"status":"ok","timestamp":1734631103735,"user_tz":-60,"elapsed":535,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}},"outputId":"e27fda29-bf9b-48f3-b45d-30ce10c2fb4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(set(), [])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"Nnvdb5H-kMqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## HuggingFace to ONNX"],"metadata":{"id":"4E0DMXWdgquf"}},{"cell_type":"markdown","source":["<img src= \"https://frenzy86.s3.eu-west-2.amazonaws.com/python/huggingonnx.png\" width=1000>\n"],"metadata":{"id":"oJCNAzqAghgR"}},{"cell_type":"markdown","source":["reference to this article:\n","https://www.philschmid.de/optimizing-transformers-with-optimum"],"metadata":{"id":"_Ho7d-ukgbsw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CF9WtjPOMmWh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b8c9501-1e20-4145-e16c-8e34774824b3","executionInfo":{"status":"ok","timestamp":1734630967889,"user_tz":-60,"elapsed":36383,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m993.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers[onnx] torch -q\n","!pip install onnx -q\n","!pip install accelerate -U -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ji2Rc21pUd-O"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"ky6PvLT4MvVO"},"source":["transformers.onnx enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects. That way you don’t have to provide the complex configuration for dynamic_axes etc."]},{"cell_type":"markdown","source":["## From PyTorch--(complicated)"],"metadata":{"id":"vZBEFnCNYbGI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vjcc2PjnMGXj"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# load model and tokenizer\n","model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")\n","\n","# export\n","torch.onnx.export(\n","                    model,\n","                    tuple(dummy_model_input.values()),\n","                    f=\"torch-model.onnx\",\n","                    input_names=['input_ids', 'attention_mask'],\n","                    output_names=['logits'],\n","                    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n","                                'attention_mask': {0: 'batch_size', 1: 'sequence'},\n","                                'logits': {0: 'batch_size', 1: 'sequence'}},\n","                    do_constant_folding=True,\n","                    opset_version=14, #chekc it if return the error of ONNXversion\n","                    )"]},{"cell_type":"markdown","metadata":{"id":"g6Zgba1HUloo"},"source":["## Exporting our checkpoint with the transformers.onnx (all in one with tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwTJ9YiBMGcy","outputId":"bea86c9b-c978-4efa-d4e0-976dc56aaae7","executionInfo":{"status":"ok","timestamp":1734631398247,"user_tz":-60,"elapsed":4273,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['input_ids', 'attention_mask'], ['last_hidden_state'])"]},"metadata":{},"execution_count":4}],"source":["from pathlib import Path\n","import transformers\n","from transformers.onnx import FeaturesManager\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","\n","# load model and tokenizer\n","model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(model_id)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# load config\n","model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model)\n","onnx_config = model_onnx_config(model.config)\n","\n","# export\n","transformers.onnx.export(\n","                        preprocessor=tokenizer,\n","                        model=model,\n","                        config=onnx_config,\n","                        opset=14,\n","                        output=Path(\"modelwithtokenizer1.onnx\")\n","                        )"]},{"cell_type":"markdown","source":["## mini inference"],"metadata":{"id":"oUVTOrT_-Z6M"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3VZzUE9O9C5","outputId":"21c2e488-3052-49df-d4c1-9e18f7846b80","executionInfo":{"status":"ok","timestamp":1734631549426,"user_tz":-60,"elapsed":780,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}],"source":["import onnxruntime\n","import numpy as np\n","\n","onnx_model_path = \"modelwithtokenizer1.onnx\"\n","session = onnxruntime.InferenceSession(onnx_model_path)\n","\n","text = \"I hate you.\"\n","\n","# Tokenize input text\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","# Get model inputs\n","inputs_onnx = {k: v.numpy() for k, v in inputs.items()}\n","outputs_onnx = session.run(None, inputs_onnx)\n","print(np.argmax(outputs_onnx[0]))"]},{"cell_type":"markdown","metadata":{"id":"x0TiUQYrNSh7"},"source":["# Export with Optimum (transformer pipeline)\n","Optimum Inference includes methods to convert vanilla Transformers models to ONNX using the ORTModelForXxx classes. To convert your Transformers model to ONNX you simply have to pass from_transformers=True to the from_pretrained() method and your model will be loaded and converted to ONNX leveraging the transformers.onnx package under the hood."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8laW_opNTVY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31cecf76-3f7c-4927-9cbc-577d9c083ebd","executionInfo":{"status":"ok","timestamp":1734631579877,"user_tz":-60,"elapsed":19649,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install optimum[onnxruntime] -q"]},{"cell_type":"markdown","metadata":{"id":"4pulDnsRNqEI"},"source":["The best part about the conversion with Optimum is that you can immediately use the model to run predictions or load it inside a pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MfkSlf3qMGfZ","outputId":"c5b3b94a-3364-47e2-d2e8-a36674db0296","executionInfo":{"status":"ok","timestamp":1734631619377,"user_tz":-60,"elapsed":19464,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('test/tokenizer_config.json',\n"," 'test/special_tokens_map.json',\n"," 'test/vocab.txt',\n"," 'test/added_tokens.json',\n"," 'test/tokenizer.json')"]},"metadata":{},"execution_count":10}],"source":["from optimum.onnxruntime import ORTModelForSequenceClassification\n","from transformers import pipeline\n","\n","model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = ORTModelForSequenceClassification.from_pretrained(\n","                                                           model_id, # onnx checkpoint\n","                                                           export=True\n","                                                           )\n","\n","# save onnx checkpoint and tokenizer\n","onnx_path = Path(\"test\")\n","model.save_pretrained(onnx_path)\n","tokenizer.save_pretrained(onnx_path)"]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","vanilla_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n","vanilla_clf(\"Could you assist me in finding my lost card?\")"],"metadata":{"id":"A4wtsYk3foTa","outputId":"9ecc820d-fa5c-4316-b66f-34abade0d8b4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734631694235,"user_tz":-60,"elapsed":239,"user":{"displayName":"Daniele Grotti","userId":"05993002232846155126"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9986515641212463}]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"gDtllLitgHbH"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}